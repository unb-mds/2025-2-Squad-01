#!/usr/bin/env python3

import os
import logging
import time
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.github_api import GitHubAPIClient, OrganizationConfig, save_json_data, load_json_data

# Configurar logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_repository_structure(
    client: GitHubAPIClient, 
    config: OrganizationConfig, 
    use_cache: bool = True,
    parallel: bool = True,  # ‚úÖ NOVO: paralelizar extra√ß√£o
    max_workers: int = 3,   # ‚úÖ NOVO: n√∫mero de threads
    repo_filter: Optional[List[str]] = None  # ‚úÖ NOVO: filtro de repos
) -> List[str]:
    """
    Extrai estrutura de arquivos usando GraphQL otimizado (queries aninhadas).
    VERS√ÉO OTIMIZADA: Reduz tempo de 20min para 2-5min!
    
    Args:
        client: Cliente da API do GitHub
        config: Configura√ß√£o da organiza√ß√£o
        use_cache: Se deve usar cache
        parallel: Se deve paralelizar extra√ß√£o (3 repos simult√¢neos)
        max_workers: N√∫mero de threads paralelas
        repo_filter: Lista de repos espec√≠ficos (None = todos)
    
    Returns:
        Lista de caminhos dos arquivos structure_{repo}.json gerados
    """
    logger.info("="*60)
    logger.info("EXTRACTING REPOSITORY STRUCTURES (GraphQL Optimized)")
    logger.info("="*60)
    logger.info(f"Optimization: Nested queries (1-3 requests per repo)")
    logger.info(f" Parallelization: {'Enabled' if parallel else 'Disabled'} ({max_workers} workers)")
    
    # Carregar reposit√≥rios filtrados
    filtered_repos = load_json_data("data/bronze/repositories_filtered.json")
    
    if not filtered_repos:
        logger.warning("‚ö†Ô∏è  No filtered repositories found. Run repository extraction first.")
        return []
    
    # Remover metadata se existir
    if isinstance(filtered_repos, list) and len(filtered_repos) > 0:
        if isinstance(filtered_repos[0], dict) and '_metadata' in filtered_repos[0]:
            filtered_repos = filtered_repos[1:]
    
    # ‚úÖ APLICAR FILTRO DE REPOS SE FORNECIDO
    if repo_filter:
        filtered_repos = [
            repo for repo in filtered_repos 
            if repo.get('name') in repo_filter
        ]
        logger.info(f" Filtering to {len(filtered_repos)} specific repos: {repo_filter}")
    
    if parallel:
        # ‚úÖ EXTRA√á√ÉO PARALELA (3 repos por vez)
        return _extract_parallel(
            client, 
            config, 
            filtered_repos, 
            use_cache, 
            max_workers
        )
    else:
        # EXTRA√á√ÉO SEQUENCIAL (original)
        return _extract_sequential(
            client, 
            config, 
            filtered_repos, 
            use_cache
        )

def _extract_sequential(
    client: GitHubAPIClient,
    config: OrganizationConfig,
    filtered_repos: List[Dict[str, Any]],
    use_cache: bool
) -> List[str]:
    """
    Extra√ß√£o sequencial (um repo por vez).
    MAIS LENTO mas mais seguro para debugging.
    """
    generated_files = []
    successful = 0
    failed = 0
    total = len(filtered_repos)
    
    start_time = time.time()
    
    for idx, repo in enumerate(filtered_repos, 1):
        result = _extract_single_repo(
            client, 
            repo, 
            use_cache, 
            idx, 
            total
        )
        
        if result:
            generated_files.append(result)
            successful += 1
        else:
            failed += 1
    
    elapsed = time.time() - start_time
    
    # Resumo final
    logger.info("\n" + "="*60)
    logger.info(f"‚úÖ Successful: {successful}")
    logger.info(f"‚ùå Failed: {failed}")
    logger.info(f"üìÅ Total files generated: {len(generated_files)}")
    logger.info(f"‚è±Ô∏è  Total time: {elapsed:.2f}s ({elapsed/60:.1f} minutes)")
    logger.info(f"‚ö° Average per repo: {elapsed/total:.1f}s")
    logger.info("="*60)
    
    return generated_files

def _extract_parallel(
    client: GitHubAPIClient,
    config: OrganizationConfig,
    filtered_repos: List[Dict[str, Any]],
    use_cache: bool,
    max_workers: int
) -> List[str]:
    """
    Extra√ß√£o paralela (3 repos simult√¢neos).
    MAIS R√ÅPIDO: Reduz tempo total em ~60%!
    """
    generated_files = []
    successful = 0
    failed = 0
    total = len(filtered_repos)
    
    logger.info(f"üöÄ Starting parallel extraction with {max_workers} workers...")
    
    start_time = time.time()
    
    # ‚úÖ PARALELIZAR com ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submeter todas as tarefas
        future_to_repo = {
            executor.submit(
                _extract_single_repo, 
                client, 
                repo, 
                use_cache, 
                idx, 
                total
            ): repo 
            for idx, repo in enumerate(filtered_repos, 1)
        }
        
        # Processar resultados conforme completam
        for future in as_completed(future_to_repo):
            repo = future_to_repo[future]
            try:
                result = future.result()
                if result:
                    generated_files.append(result)
                    successful += 1
                else:
                    failed += 1
            except Exception as e:
                logger.error(f"   ‚ùå Exception for {repo.get('name')}: {str(e)}")
                failed += 1
    
    elapsed = time.time() - start_time
    
    # Resumo final
    logger.info("\n" + "="*60)
    logger.info(f"‚úÖ Successful: {successful}")
    logger.info(f"‚ùå Failed: {failed}")
    logger.info(f"üìÅ Total files generated: {len(generated_files)}")
    logger.info(f"‚è±Ô∏è  Total time: {elapsed:.2f}s ({elapsed/60:.1f} minutes)")
    logger.info(f"‚ö° Average per repo: {elapsed/total:.1f}s")
    logger.info(f"üöÄ Speedup vs sequential: ~{3/max_workers:.1f}x faster")
    logger.info("="*60)
    
    return generated_files

def _extract_single_repo(
    client: GitHubAPIClient,
    repo: Dict[str, Any],
    use_cache: bool,
    idx: int,
    total: int
) -> Optional[str]:
    """
    Extrai estrutura de um √∫nico reposit√≥rio.
    
    Args:
        client: Cliente GitHub API
        repo: Dados do reposit√≥rio
        use_cache: Se deve usar cache
        idx: √çndice atual (para progresso)
        total: Total de repos
    
    Returns:
        Path do arquivo gerado ou None se falhou
    """
    if not repo or not isinstance(repo, dict):
        logger.warning(f"Skipping invalid repo entry")
        return None
    
    repo_name = repo.get('name', 'unknown')
    full_name = repo.get('full_name', repo_name)
    default_branch = repo.get('default_branch', 'main')
    
    # Extrair owner do full_name
    if '/' not in full_name:
        logger.warning(f"Skipping {repo_name}: invalid full_name format")
        return None
    
    owner, name_only = full_name.split('/', 1)
    
    # ‚úÖ PROGRESSO VISUAL
    progress_pct = (idx / total) * 100
    estimated_remaining = (total - idx) * 15  # 15s por repo (estimativa otimista)
    
    logger.info(f"\n[{idx}/{total}] ({progress_pct:.1f}%) üìÇ {repo_name}")
    logger.info(f"   ‚è±Ô∏è  Est. time remaining: ~{estimated_remaining}s ({estimated_remaining//60}min)")
    
    try:
        # ‚úÖ VERIFICAR SE J√Å EXISTE (evitar re-extrair)
        output_file = f"data/bronze/structure_{repo_name}.json"
        if use_cache and os.path.exists(output_file):
            logger.info(f"   ‚úì Structure already exists (using cached)")
            return output_file
        
        # ‚úÖ USAR M√âTODO OTIMIZADO (queries aninhadas)
        structure = client.graphql_repository_tree_optimized(
            owner=owner,
            repo=name_only,
            branch=default_branch,
            use_cache=use_cache
        )
        
        # Validar dados m√≠nimos
        if not structure or not structure.get('tree'):
            logger.warning(f"   ‚ö†Ô∏è  No files found in {repo_name}")
            return None
        
        total_items = structure.get('total_items', len(structure['tree']))
        extraction_time = structure.get('extraction_time_seconds', 0)
        
        if total_items == 0:
            logger.warning(f"   ‚ö†Ô∏è  Empty tree for {repo_name}")
            return None
        
        # Adicionar metadados do reposit√≥rio
        structure['repository_metadata'] = {
            'id': repo.get('id'),
            'full_name': full_name,
            'description': repo.get('description'),
            'created_at': repo.get('created_at'),
            'updated_at': repo.get('updated_at'),
            'language': repo.get('language'),
            'size': repo.get('size'),
            'stars': repo.get('stargazers_count', 0),
            'forks': repo.get('forks_count', 0),
            'open_issues': repo.get('open_issues_count', 0),
        }
        
        # Salvar
        output_file = save_json_data(
            structure,
            output_file,
            timestamp=False
        )
        
        logger.info(f"   ‚úÖ Saved: {output_file}")
        logger.info(f"   üìä Items: {total_items} | ‚è±Ô∏è  Time: {extraction_time:.1f}s")
        
        return output_file
        
    except Exception as e:
        logger.error(f"   ‚ùå Error extracting {repo_name}: {str(e)}")
        return None